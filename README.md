<div align="center">

# <img src="assets/loopy_spa.png" alt="Logo" width="35"/> <span style="color: #FF7096;">SPA</span>: 3D <span style="color: #FF7096;">SP</span>atial-<span style="color: #FF7096;">A</span>wareness Enables Effective Embodied Representation


[![python](https://img.shields.io/badge/-Python_3.9_%7C_3.10_%7C_3.11-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)
[![black](https://img.shields.io/badge/Code%20Style-Black-black.svg?labelColor=gray)](https://black.readthedocs.io/en/stable/)
[![isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)
[![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)

[**Project Page**](https://haoyizhu.github.io/spa/) | [**Paper**](https://haoyizhu.github.io/spa/static/images/paper.pdf) | [**arXiv**](https://arxiv.org/abs/2410.08208) | [**HuggingFace Model**](https://huggingface.co/HaoyiZhu/SPA) | [**Real-World Codebase**](https://github.com/HaoyiZhu/RealRobot) | [**Twitter/X**](https://x.com/HaoyiZhu/status/1844675411760013471) | [**YouTube Video**](https://www.youtube.com/watch?v=LS2R-kBxxwY)

[Haoyi Zhu](https://www.haoyizhu.site/), [Honghui Yang](https://hhyangcs.github.io/), [Yating Wang](https://scholar.google.com/citations?hl=zh-CN&user=5SuBWh0AAAAJ),  [Jiange Yang](https://yangjiangeyjg.github.io/), [Liming Wang](https://wanglimin.github.io/), [Tong He](http://tonghe90.github.io/)
</div>

![teaser](assets/teaser.gif)

**SPA** is a novel representation learning framework that emphasizes the importance of **3D spatial awareness in embodied AI**. It leverages **differentiable neural rendering** on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding. We also present the most comprehensive evaluation of embodied representation learning to date, covering **268 tasks** across **8 simulators** with diverse policies in both single-task and language-conditioned multi-task scenarios.

:partying_face: **NEWS**: 

- *Jan. 2025:* SPA is accepted by ICLR 2025!
- *Oct. 2024:* Codebase and pre-trained checkpoints are released! Paper is available on [arXiv](https://arxiv.org/abs/2410.08208).

## :clipboard: Contents

- [Project Structure](#telescope-project-structure)
- [Installation](#installation)
- [Usage](#star2-usage)
- [Pre-Training](#rocket-pre-training)
- [SPA Large-Scale Evaluation](#bulb-spa-large-scale-evaluation)
- [Gotchas](#tada-gotchas)
- [License](#books-license)
- [Acknowledgement](#sparkles-acknowledgement)
- [Citation](#pencil-citation)

## :telescope: Project Structure

Our codebase draws significant inspiration from the excellent [Lightning Hydra Template](https://github.com/ashleve/lightning-hydra-template). The directory structure of this project is organized as follows:

<details>
<summary><b>Show directory structure</b></summary>

```
‚îú‚îÄ‚îÄ .github                   <- Github Actions workflows
‚îÇ
‚îú‚îÄ‚îÄ configs                   <- Hydra configs
‚îÇ   ‚îú‚îÄ‚îÄ callbacks                         <- Callbacks configs
‚îÇ   ‚îú‚îÄ‚îÄ data                              <- Data configs
‚îÇ   ‚îú‚îÄ‚îÄ debug                             <- Debugging configs
‚îÇ   ‚îú‚îÄ‚îÄ experiment                        <- Experiment configs
‚îÇ   ‚îú‚îÄ‚îÄ extras                            <- Extra utilities configs
‚îÇ   ‚îú‚îÄ‚îÄ hydra                             <- Hydra configs
‚îÇ   ‚îú‚îÄ‚îÄ local                             <- Local configs
‚îÇ   ‚îú‚îÄ‚îÄ logger                            <- Logger configs
‚îÇ   ‚îú‚îÄ‚îÄ model                             <- Model configs
‚îÇ   ‚îú‚îÄ‚îÄ paths                             <- Project paths configs
‚îÇ   ‚îú‚îÄ‚îÄ trainer                           <- Trainer configs
|   |
‚îÇ   ‚îî‚îÄ‚îÄ train.yaml            <- Main config for training
‚îÇ
‚îú‚îÄ‚îÄ data                   <- Project data
‚îÇ
‚îú‚îÄ‚îÄ logs                   <- Logs generated by hydra and lightning loggers
‚îÇ
‚îú‚îÄ‚îÄ scripts                <- Shell or Python scripts
|
‚îú‚îÄ‚îÄ spa                    <- Source code of SPA
‚îÇ   ‚îú‚îÄ‚îÄ data                     <- Data scripts
‚îÇ   ‚îú‚îÄ‚îÄ models                   <- Model scripts
‚îÇ   ‚îú‚îÄ‚îÄ utils                    <- Utility scripts
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ train.py                 <- Run SPA pre-training
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                <- List of files ignored by git
‚îú‚îÄ‚îÄ .project-root             <- File for inferring the position of project root directory
‚îú‚îÄ‚îÄ requirements.txt          <- File for installing python dependencies
‚îú‚îÄ‚îÄ setup.py                  <- File for installing project as a package
‚îî‚îÄ‚îÄ README.md
```

</details>

## :hammer: Installation

> ‚ö†Ô∏è *Warning*: We have observed that using latest PyTorch versions (e.g., PyTorch 2.6) can lead to different feature maps compared to our original experiments.
Currently, we do not know the exact reason for these discrepancies, nor can we confirm whether these differences will impact the final evaluation results.
For **reproducibility purposes**, we strongly recommend using **PyTorch 2.2.1**, which is the version used in our original development and testing.
If you choose to use a newer version, please be aware of this potential issue and proceed with caution.

<details>
<summary><b>Basics</b></summary>

```console
# clone project
git clone https://github.com/HaoyiZhu/SPA.git
cd SPA

# crerate conda environment
conda create -n spa python=3.11 -y
conda activate spa

# install PyTorch, please refer to https://pytorch.org/ for other CUDA versions
# e.g. cuda 11.8:
pip3 install torch==2.2.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
# install basic packages
pip3 install -r requirements.txt
```
</details>

<details>
<summary><b>SPA</b></summary>

```console
# (optional) if you want to use SPA's volume decoder
cd libs/spa-ops
pip install -e .
cd ../..

# install SPA, so that you can import from anywhere
pip install -e .
```
</details>

## :star2: Usage

<details open>
  <summary><b>Example of Using SPA Pre-trained Encoder </b></summary>

We provide pre-trained SPA weights for feature extraction. The checkpoints are available on [ü§óHugging Face](https://huggingface.co/HaoyiZhu/SPA). You don't need to manually download the weights, as SPA will automatically handle this if needed.

```python
import torch

from spa.models import spa_vit_base_patch16, spa_vit_large_patch16

image = torch.rand((1, 3, 224, 224))  # range in [0, 1]

# Example usage of SPA-Large (recommended)
# or you can use `spa_vit_base_patch16` for SPA-base
model = spa_vit_large_patch16(pretrained=True)
model.eval()

# Freeze the model
model.freeze()

# (Recommended) move to CUDA
image = image.cuda()
model = model.cuda()

# Obtain the [CLS] token
cls_token = model(image)  # torch.Size([1, 1024])

# Obtain the reshaped feature map concatenated with [CLS] token
feature_map_cat_cls = model(
    image, feature_map=True, cat_cls=True
)  # torch.Size([1, 2048, 14, 14])

# Obtain the reshaped feature map without [CLS] token
feature_map_wo_cls = model(
    image, feature_map=True, cat_cls=False
)  # torch.Size([1, 1024, 14, 14])
```

> **Note:** The inputs will be automatically resized to `224 x 224` and normalized within the [SPA ViT encoder](spa/models/components/img_backbones/vit.py#L69).

</details>



## :rocket: Pre-Training

<details>
  <summary><b>Example of Pre-Training on ScanNet </b></summary>

We give an example on pre-training SPA on the [ScanNet](http://www.scan-net.org/) v2 dataset.

1) Prepare the dataset
    - Download the [ScanNet](http://www.scan-net.org/) v2 dataset.
    - Pre-process and extract RGB-D images following [PonderV2](https://github.com/OpenGVLab/PonderV2/blob/main/docs/data_preparation.md#scannet-v2). The preprocessed data should be put under `data/scannet/`.
    - Pre-generate metadata for fast data loading. The following command will generate metadata under `data/scannet/metadata`.
        ```console
        python scripts/generate_scannet_metadata.py
        ```

2) Run the following command for pre-training. Remember to modify hyper-parameters such as number of nodes and GPU devices according to your machines.
    ```console
    python spa/train.py experiment=spa_pretrain_vitl trainer.num_nodes=5 trainer.devices=8
    ```

</details>

## :bulb: SPA Large-Scale Evaluation

<details>
<summary><b>VC-1 Evaluation</b></summary>

We evaluate on the VC-1's MetaWorld, Adroit, DMControl, and TriFinger benchmarks. Additionally, we have a [forked version of the repository](https://github.com/xiaoxiao0406/eai-vc.git) that includes code and configuration for evaluating SPA.

1) Clone the [forked VC-1 repo](https://github.com/xiaoxiao0406/eai-vc.git), or you can use the [submodule](evaluation/eai-vc) by `git submodule update --init --recursive` and `cd evaluation/eai-vc`.
Then, please follow the instructions in the [CortexBench README](https://github.com/facebookresearch/eai-vc/blob/main/cortexbench/README.md) to set up the MuJoCo and TriFinger environments, as well as download the required datasets.
   
2) Create a configuration for spa `<spa_model>.yaml`(e.g., using SPA-Large as in [spa_vit_large.yaml](https://github.com/xiaoxiao0406/eai-vc/blob/main/vc_models/src/vc_models/conf/model/spa_vit_large.yaml)) in [<vc-1_path>/vc_models/src/vc_models/conf/model](https://github.com/xiaoxiao0406/eai-vc/tree/main/vc_models/src/vc_models/conf/model).

3) To run the VC-1 evaluation for spa, specify the model config as a parameter (embedding=<spa_model>) for each of the benchmarks in [cortexbench](https://github.com/xiaoxiao0406/eai-vc/tree/main/cortexbench).
</details> 

<details>
<summary><b>LIBERO Evaluation</b></summary>

Please first run `git submodule update --init --recursive`. Then install the LIBERO enviornment:

```console
cd evaluations/LIBERO
pip3 install -r requirements.txt
pip3 install -e .
```

Then you have to download LIBERO datasets:
```console
python benchmark_scripts/download_libero_datasets.py
```

Then you can choose:

- `BENCHMARK` from `[LIBERO_SPATIAL, LIBERO_OBJECT, LIBERO_GOAL, LIBERO_90, LIBERO_10]`

then run the following:

```console
export CUDA_VISIBLE_DEVICES=GPU_ID && \
export MUJOCO_EGL_DEVICE_ID=GPU_ID && \
python libero/lifelong/main.py seed=SEED \
                               benchmark_name=BENCHMARK \
                               policy=bc_transformer_policy \
                               lifelong=multitask \
                               policy/image_encoder=spa_encoder.yaml
```
Note that in SPA paper, we remove all the data augmentations since we aim to produce a simple and fair setting instead of training a SOTA policy. To do so, you could run the following:
```console
export CUDA_VISIBLE_DEVICES=GPU_ID && \
export MUJOCO_EGL_DEVICE_ID=GPU_ID && \
python libero/lifelong/main.py seed=SEED \
                               benchmark_name=BENCHMARK \
                               policy=bc_transformer_policy \
                               lifelong=multitask \
                               policy/image_encoder=spa_encoder.yaml \
                               policy/data_augmentation@policy.color_aug=identity_aug.yaml \
                               policy/data_augmentation@policy.translation_aug=identity_aug.yaml
```
> Actually, in SPA's experiments, for speed consideration, we use only 20 demos for each task. To do so, you may need to manually modify the datasets. Moreover, SPA only trains for 25 epochs.


If you encounter this error, it is due to LIBERO's numpy version.
```
AttributeError: module 'numpy' has no attribute 'bool'.
`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
```
You can downgrade your numpy version:
```console
pip install "numpy<1.24"
```

For more details, please refer to LIBERO's official documentation.

</details> 

<details>
<summary><b>Camera Pose Evaluation</b></summary>

To reproduce the camera pose evaluation, we have open-sourced the code in [evaluations/probe3d](evaluations/probe3d). Please first run `git submodule update --init --recursive` and `cd evaluations/probe3d`. Then follow the instructions in [probe3d](https://github.com/HaoyiZhu/probe3d/blob/main/data_processing/README.md) to prepare the **NAVI** dataset. Finally, run the following command to evaluate SPA:

```console
python evaluate_navi_camera_pose.py
```
</details> 

<details>
<summary><b>Featuremap Visualization</b></summary>

> ‚ö†Ô∏è *Warning*: We have observed that using latest PyTorch versions (e.g., PyTorch 2.6) can lead to different feature maps compared to our original experiments.
Currently, we do not know the exact reason for these discrepancies, nor can we confirm whether these differences will impact the final evaluation results.
For **reproducibility purposes**, we strongly recommend using **PyTorch 2.2.1**, which is the version used in our original development and testing.
If you choose to use a newer version, please be aware of this potential issue and proceed with caution.

To reproduce the feature map visualization results, you can run with:

```console
python scripts/visualize_featuremap.py --image_folder assets/feature_map_vis
```
</details> 

## :tada: Gotchas

<details>
<summary><b> Override any config parameter from command line </b></summary>

This codebase is based on [Hydra](https://github.com/facebookresearch/hydra), which allows for convenient configuration overriding:
```console
python src/train.py trainer.max_epochs=20 seed=300
```
> **Note**: You can also add new parameters with `+` sign.
```console
python src/train.py +some_new_param=some_new_value
```

</details>

<details>
<summary><b>Train on CPU, GPU, multi-GPU and TPU</b></summary>

```console
# train on CPU
python src/train.py trainer=cpu

# train on 1 GPU
python src/train.py trainer=gpu

# train on TPU
python src/train.py +trainer.tpu_cores=8

# train with DDP (Distributed Data Parallel) (4 GPUs)
python src/train.py trainer=ddp trainer.devices=4

# train with DDP (Distributed Data Parallel) (8 GPUs, 2 nodes)
python src/train.py trainer=ddp trainer.devices=4 trainer.num_nodes=2

# simulate DDP on CPU processes
python src/train.py trainer=ddp_sim trainer.devices=2

# accelerate training on mac
python src/train.py trainer=mps
```

</details>

<details>
<summary><b>Train with mixed precision</b></summary>

```console
# train with pytorch native automatic mixed precision (AMP)
python src/train.py trainer=gpu +trainer.precision=16
```

</details>

<details>
<summary><b>Use different tricks available in Pytorch Lightning</b></summary>

```yaml
# gradient clipping may be enabled to avoid exploding gradients
python src/train.py trainer.gradient_clip_val=0.5

# run validation loop 4 times during a training epoch
python src/train.py +trainer.val_check_interval=0.25

# accumulate gradients
python src/train.py trainer.accumulate_grad_batches=10

# terminate training after 12 hours
python src/train.py +trainer.max_time="00:12:00:00"
```

> **Note**: PyTorch Lightning provides about [40+ useful trainer flags](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags).

</details>

<details>
<summary><b>Easily debug</b></summary>

```console
# runs 1 epoch in default debugging mode
# changes logging directory to `logs/debugs/...`
# sets level of all command line loggers to 'DEBUG'
# enforces debug-friendly configuration
python src/train.py debug=default

# run 1 train, val and test loop, using only 1 batch
python src/train.py debug=fdr

# print execution time profiling
python src/train.py debug=profiler

# try overfitting to 1 batch
python src/train.py debug=overfit

# raise exception if there are any numerical anomalies in tensors, like NaN or +/-inf
python src/train.py +trainer.detect_anomaly=true

# use only 20% of the data
python src/train.py +trainer.limit_train_batches=0.2 \
+trainer.limit_val_batches=0.2 +trainer.limit_test_batches=0.2
```

> **Note**: Visit [configs/debug/](configs/debug/) for different debugging configs.

</details>

<details>
<summary><b>Resume training from checkpoint</b></summary>

```yaml
python src/train.py ckpt_path="/path/to/ckpt/name.ckpt"
```

> **Note**: Checkpoint can be either path or URL.

> **Note**: Currently loading ckpt doesn't resume logger experiment, but it will be supported in future Lightning release.

</details>

<details>
<summary><b>Create a sweep over hyperparameters</b></summary>

```console
# this will run 9 experiments one after the other,
# each with different combination of seed and learning rate
python src/train.py -m seed=100,200,300 model.optimizer.lr=0.0001,0.00005,0.00001
```

> **Note**: Hydra composes configs lazily at job launch time. If you change code or configs after launching a job/sweep, the final composed configs might be impacted.

</details>

<details>
<summary><b>Execute all experiments from folder</b></summary>

```console
python src/train.py -m 'exp_maniskill2_act_policy/maniskill2_task@maniskill2_task=glob(*)'
```

> **Note**: Hydra provides special syntax for controlling behavior of multiruns. Learn more [here](https://hydra.cc/docs/next/tutorials/basic/running_your_app/multi-run). The command above executes all task experiments from [configs/exp_maniskill2_act_policy/maniskill2_task](configs/experiment/).

</details>

<details>
<summary><b>Execute run for multiple different seeds</b></summary>

```console
python src/train.py -m seed=100,200,300 trainer.deterministic=True
```

> **Note**: `trainer.deterministic=True` makes pytorch more deterministic but impacts the performance.

</details>

For more instructions, refer to the official documentation for [Pytorch Lightning](https://github.com/Lightning-AI/pytorch-lightning), [Hydra](https://github.com/facebookresearch/hydra), and [Lightning Hydra Template](https://github.com/ashleve/lightning-hydra-template).

## :books: License

This repository is released under the [MIT license](LICENSE).

## :sparkles: Acknowledgement

Our work is primarily built upon [PointCloudMatters](https://github.com/HaoyiZhu/PointCloudMatters), [PonderV2](https://github.com/OpenGVLab/PonderV2), [UniPAD](https://github.com/Nightmare-n/UniPAD), [Pytorch Lightning](https://github.com/Lightning-AI/pytorch-lightning), [Hydra](https://github.com/facebookresearch/hydra), [Lightning Hydra Template](https://github.com/ashleve/lightning-hydra-template), [RLBench](https://github.com/stepjam/RLBench), [PerAct](https://github.com/peract/peract), [LIBERO](https://github.com/Lifelong-Robot-Learning/LIBERO), [Meta-Wolrd](https://github.com/Farama-Foundation/Metaworld), [ACT](https://github.com/tonyzhaozh/act), [Diffusion Policy](https://github.com/real-stanford/diffusion_policy), [DP3](https://github.com/YanjieZe/3D-Diffusion-Policy), [TIMM](https://github.com/huggingface/pytorch-image-models), [VC1](https://github.com/facebookresearch/eai-vc), [R3M](https://github.com/facebookresearch/r3m). We extend our gratitude to all these authors for their generously open-sourced code and their significant contributions to the community.

Contact [Haoyi Zhu](https://www.haoyizhu.site/) if you have any questions or suggestions.

## :pencil: Citation

```bib
@article{zhu2024spa,
    title = {SPA: 3D Spatial-Awareness Enables Effective Embodied Representation},
    author = {Zhu, Haoyi and and Yang, Honghui and Wang, Yating and Yang, Jiange and Wang, Limin and He, Tong},
    journal = {arXiv preprint arxiv:2410.08208},
    year = {2024},
}
```
